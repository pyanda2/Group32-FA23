{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eac65b6-2953-4adc-83f7-d5605ad4bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "path_to_train = \"/Users/bilbaothanos14/Documents/GitHub/Group32-FA23/archive/sign_mnist_train.csv\"\n",
    "path_to_test= \"/Users/bilbaothanos14/Documents/GitHub/Group32-FA23/archive/sign_mnist_test.csv\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(path_to_train)\n",
    "test_df = pd.read_csv(path_to_test)\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "del train_df['label']\n",
    "del test_df['label']\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(y_train)\n",
    "y_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "x_train = train_df.values\n",
    "x_test = test_df.values\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcd9fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bilbaothanos14/.zshenv:export:1: not valid in this context: /Library/Java/JavaVirtualMachines/jdk-19.jdk/Contents/Home\n",
      "Requirement already satisfied: seaborn in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9eaec60-a348-4a3d-a2c2-ad09c4e7784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False, \n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7f0e92-6009-4361-82d9-a7d852c4f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 17:51:44.218202: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-10 17:51:44.218642: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9486f016-82d8-4655-8154-c1bc41795911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 75)       300       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 75)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 50)       200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 25)         100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 25)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 17:51:46.188844: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-10-10 17:51:46.849155: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - ETA: 0s - loss: 1.0709 - accuracy: 0.6644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 17:52:16.390934: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 45s 170ms/step - loss: 1.0709 - accuracy: 0.6644 - val_loss: 3.3348 - val_accuracy: 0.0577\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.1978 - accuracy: 0.9364 - val_loss: 0.9290 - val_accuracy: 0.6842\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 30s 140ms/step - loss: 0.0886 - accuracy: 0.9721 - val_loss: 0.1870 - val_accuracy: 0.9424\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 29s 135ms/step - loss: 0.0612 - accuracy: 0.9807 - val_loss: 0.0595 - val_accuracy: 0.9801\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 30s 140ms/step - loss: 0.0460 - accuracy: 0.9859 - val_loss: 0.0250 - val_accuracy: 0.9918\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 30s 137ms/step - loss: 0.0340 - accuracy: 0.9890 - val_loss: 0.0452 - val_accuracy: 0.9819\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 29s 133ms/step - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.2480 - val_accuracy: 0.8982\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 30s 139ms/step - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.0199 - val_accuracy: 0.9954\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 29s 136ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0471 - val_accuracy: 0.9822\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 30s 139ms/step - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.0390 - val_accuracy: 0.9875\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 29s 136ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.0392 - val_accuracy: 0.9862\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 28s 132ms/step - loss: 0.0206 - accuracy: 0.9925 - val_loss: 0.0116 - val_accuracy: 0.9953\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 29s 135ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0318 - val_accuracy: 0.9877\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 29s 135ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.2968 - val_accuracy: 0.9264\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 29s 137ms/step - loss: 0.0200 - accuracy: 0.9929 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 30s 142ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0665 - val_accuracy: 0.9769\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 30s 139ms/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 0.0319 - val_accuracy: 0.9866\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 30s 139ms/step - loss: 0.0182 - accuracy: 0.9938 - val_loss: 0.0278 - val_accuracy: 0.9934\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 31s 143ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 1.1580 - val_accuracy: 0.7911\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 30s 139ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0200 - val_accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(datagen.flow(x_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (x_test, y_test))\n",
    "\n",
    "model.save('weights/smnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e621ac4-c9a7-42a4-9499-394fe259a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb366af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bilbaothanos14/.zshenv:export:1: not valid in this context: /Library/Java/JavaVirtualMachines/jdk-19.jdk/Contents/Home\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.7-cp39-cp39-macosx_11_0_universal2.whl (53.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: matplotlib in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from mediapipe) (3.5.2)\n",
      "Requirement already satisfied: absl-py in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from mediapipe) (1.0.0)\n",
      "Requirement already satisfied: numpy in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from mediapipe) (1.21.6)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from mediapipe) (2.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from mediapipe) (3.20.1)\n",
      "Collecting sounddevice>=0.4.4\n",
      "  Downloading sounddevice-0.4.6-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl (41.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: CFFI>=1.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.0)\n",
      "Requirement already satisfied: six in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from absl-py->mediapipe) (1.15.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.4.3)\n",
      "Requirement already satisfied: pycparser in /Users/bilbaothanos14/miniforge3/lib/python3.9/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Installing collected packages: opencv-contrib-python, sounddevice, mediapipe\n",
      "Successfully installed mediapipe-0.10.7 opencv-contrib-python-4.8.1.78 sounddevice-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba3892d-cda5-4478-a2ab-7307d321af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "OpenCV: not authorized to capture video (status 0), requesting...\n",
      "OpenCV: camera failed to properly initialize!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 9\u001b[0m h, w, c \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m     11\u001b[0m analysisframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m letterpred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model = load_model('weights/smnist.h5')\n",
    "\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "_, frame = cap.read()\n",
    "h, w, c = frame.shape\n",
    "\n",
    "analysisframe = ''\n",
    "letterpred = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5041b4bc-5207-4063-8de3-295cbd6cac69",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     pixeldata \u001b[38;5;241m=\u001b[39m pixeldata \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     58\u001b[0m     pixeldata \u001b[38;5;241m=\u001b[39m pixeldata\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m framergb \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m result \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(framergb)\n\u001b[1;32m     62\u001b[0m hand_landmarks \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# This code is modified from the article to combine the videostream and picture code\n",
    "# press space whenever to snap a picture, then press esc to stop\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        analysisframe = frame\n",
    "        framergbanalysis = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2RGB)\n",
    "        resultanalysis = hands.process(framergbanalysis)\n",
    "        hand_landmarksanalysis = resultanalysis.multi_hand_landmarks\n",
    "        if hand_landmarksanalysis:\n",
    "            for handLMsanalysis in hand_landmarksanalysis:\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lmanalysis in handLMsanalysis.landmark:\n",
    "                    x, y = int(lmanalysis.x * w), int(lmanalysis.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                y_min -= 20\n",
    "                y_max += 20\n",
    "                x_min -= 20\n",
    "                x_max += 20 \n",
    "\n",
    "        analysisframe = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2GRAY)\n",
    "        analysisframe = analysisframe[y_min:y_max, x_min:x_max]\n",
    "        analysisframe = cv2.resize(analysisframe,(28,28))\n",
    "\n",
    "        nlist = []\n",
    "        rows,cols = analysisframe.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                k = analysisframe[i,j]\n",
    "                nlist.append(k)\n",
    "        \n",
    "        datan = pd.DataFrame(nlist).T\n",
    "        colname = []\n",
    "        for val in range(784):\n",
    "            colname.append(val)\n",
    "        datan.columns = colname\n",
    "\n",
    "        pixeldata = datan.values\n",
    "        pixeldata = pixeldata / 255\n",
    "        pixeldata = pixeldata.reshape(-1,28,28,1)\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "    if hand_landmarks:\n",
    "        for handLMs in hand_landmarks:\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            y_min -= 20\n",
    "            y_max += 20\n",
    "            x_min -= 20\n",
    "            x_max += 20\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            mp_drawing.draw_landmarks(frame, handLMs, mphands.HAND_CONNECTIONS)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c1a437-a650-4f6b-9cb7-3064c751dd25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pixeldata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mpixeldata\u001b[49m)\n\u001b[1;32m      2\u001b[0m predarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(prediction[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m letter_prediction_dict \u001b[38;5;241m=\u001b[39m {letterpred[i]: predarray[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(letterpred))}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pixeldata' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(pixeldata)\n",
    "predarray = np.array(prediction[0])\n",
    "letter_prediction_dict = {letterpred[i]: predarray[i] for i in range(len(letterpred))}\n",
    "predarrayordered = sorted(predarray, reverse=True)\n",
    "high1 = predarrayordered[0]\n",
    "high2 = predarrayordered[1]\n",
    "high3 = predarrayordered[2]\n",
    "for key,value in letter_prediction_dict.items():\n",
    "    if value==high1:\n",
    "        print(\"Predicted Character 1: \", key)\n",
    "        print('Confidence 1: ', 100*value)\n",
    "    elif value==high2:\n",
    "        print(\"Predicted Character 2: \", key)\n",
    "        print('Confidence 2: ', 100*value)\n",
    "    elif value==high3:\n",
    "        print(\"Predicted Character 3: \", key)\n",
    "        print('Confidence 3: ', 100*value)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb5453-a06c-4cc3-b8ea-fe15b44e5959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
